# 9. 声明式地升级应用
## 9.1 更新运行在pod内的应用程序
### 9.1.1 删除旧版本pod,使用新版本pod替换
如果我们使用的是ReplicationController,我们考虑修改模板v2,手动删除podv1,Replication Controller会帮我们把v2启动起来。
### 9.1.2 先创建新版本pod再删除旧版本pod
刚才9.1.1那种短暂的服务停止不能被接受，并且你的应用支持多个版本同时对外的服务，那么可以先新创建pod再删除原来的pod.这会需要更多的硬件资源，因为在短暂的时间内，同时运行2倍数量的pod.
**从旧版本换到新版本**
蓝绿部署
pod通常是通过service暴露，在运行新版本pod之前，service只将流量切换到初始版本的pod,一旦新版本的pod被创建并正常运行之后，就可以修改服务的标签选择器，将service的流量切换到新的pod。在切换之后，一旦确定新版本的功能运行正常，就可以通过删除旧的ReplicationController来删除旧版本的pod。

**执行滚动升级操作**
还可以通过滚动升级的操作来替换原来的pod,而不是同时创建所有新的pod，并一并删除所有旧的pod.可以通过逐步对旧版本的ReplicationController缩容并对新版本进行扩容。但是这个滚动升级非常麻烦，实际上k8s可以仅仅一条命令来实现滚动升级。

## 9.2 使用ReplicationController实现自动的滚动升级
滚动升级主要解决B/G升级中一次要使用加倍的服务器资源的问题。他可以实现3个服务器，老的那边减少一个，新的那边增加一个的方式进行升级。
### 9.2.1 运行第一个版本的应用
app.js
```
const http = require('http');
const os = require('os');

console.log("Kubia server starting...");

var handler = function(request, response) {
  console.log("Received request from " + request.connection.remoteAddress);
  response.writeHead(200);
  response.end("This is v1 running in pod " + os.hostname() + "\n");
};

var www = http.createServer(handler);
www.listen(8080);
```
**使用单个YAML**文件运行并通过Service暴露
kubia-rc-and-service-v1.yaml
```
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v1
        name: nodejs
---
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  type: LoadBalancer
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
```
在minikube中不支持LoadBalancer的k8s集群，需要使用Service节点端口方式来访问应用由于使用下面命令查看
```
minikube service kubia
```
### 9.2.2 使用kubectl来执行滚动升级
但是现在这个命令已经不再支持
```
k rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2
Error: unknown command "rolling-update" for "kubectl"
Run 'kubectl --help' for usage.
```
### 9.2.3 为什么kubectl rolling-update已经过时
- 他修改了pod和ReplicationController的标签
- 他是客户端触发的，客户端可能会面临网络问题，导致升级处于中间状态。
解决这个问题我们引入了Deployment这个资源。

## 9.3 使用Deployment申明式地升级应用
Deployment是一个更高级的资源。用于部署应用程序并以声明式的方式升级。而不是像ReplicationController来复制和管理pod。Deployment并不直接管理pod，而是通过管理ReplicaSet来管理pod.
### 9.3.1 创建一个Deployment
kubia-deployment-v1.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v1
        name: nodejs
  selector:
    matchLabels:
      app: kubia
```
**创建Deployment资源**
删除之前的replicationcontroller，但是我们还留了kubia Service一条狗命
```
kubectl delete rc --all
```
创建deployment，使用--record选项，这个选项会记录历史版本号，在之后的操作中非常有用
```
kubectl create -f kubia-deployment-v1.yaml --record
```
**展示Deployment滚动过程中的状态**
这个命令是用来查看滚动升级中的状态用的，只是查询状态不是升级
```
kubectl rollout status deployment kubia
deployment "kubia" successfully rolled out
```
下面可以看到Deployment管理的ReplicaSet和pod，他们都具有相同的hash值
查看deployment管理的pod
```
NAME                     READY   STATUS    RESTARTS   AGE
kubia-74967b5695-hsn8s   1/1     Running   0          9m15s
kubia-74967b5695-k8jjx   1/1     Running   0          9m15s
kubia-74967b5695-wpj75   1/1     Running   0          9m15s
```
查看deployment管理的replicaset
```
k get replicaset
NAME               DESIRED   CURRENT   READY   AGE
kubia-74967b5695   3         3         3       19m
```
### 9.3.2 升级Deployment
**不同的Deployment升级策略**
- RollingUpdate
滚动升级，创建一个新的资源，关闭一个新的资源，最后达到收敛状态
- Recreate
新服务启用前，完全停用旧服务。
**演示如何减慢滚动升级速度**
```
kubectl patch deployment kubia -p '{"spec":{"minReadySeconds":10}}'
deployment.apps/kubia patched
```
开始升级
```
kubectl set image deployment kubia nodejs=luksa/kubia:v2
deployment.apps/kubia image updated
```
欣赏下升级的过程
```
kubectl rollout status deployment kubia
Waiting for deployment "kubia" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "kubia" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "kubia" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "kubia" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "kubia" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "kubia" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "kubia" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "kubia" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "kubia" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "kubia" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "kubia" rollout to finish: 1 old replicas are pending termination...
deployment "kubia" successfully rolled out
```
观察升级结果,这个kubia:v2是会打印v2这样的样式的。
```
minikube service kubia
```
**Deployment的优点**
Deployment中通过更改Deployment资源中的pod模板，应用程序已经被升级为一个更新的版本，仅仅通过更改一个字段而已。这个升级是个k8s上的控制器处理和完成的。
### 9.3.3 回滚Deployment
v3是一个有问题的版本，他http调用超过5次之后，就会出现问题。所以我们先升级到v3,然后降级为v2
升级
```
kubectl set image deployment kubia nodejs=luksa/kubia:v3
deployment.apps/kubia image updated
```
降级
```
kubectl rollout undo deployment kubia
deployment.apps/kubia rolled back
```
**显示Deployment的滚动升级历史**
```
kubectl rollout history deployment kubia
deployment.apps/kubia
REVISION  CHANGE-CAUSE
1         kubectl create --filename=kubia-deployment-v1.yaml --record=true
3         kubectl create --filename=kubia-deployment-v1.yaml --record=true
4         kubectl create --filename=kubia-deployment-v1.yaml --record=true
```
**回滚到一个特定的Deployment版本**
```
kubectl rollout history kubia --to-reversion=1
```
### 9.3.4控制滚动升级速率
- maxSurge 决定了Deployment配置中期望副本之外，最多允许超出的pod实例的数量。默认值为25%。如果期望副本是4，那么在升级过程中不超过5个pod在运行。
- maxUnavailable决定在滚动升级期间，对于期望副本数能够允许有多少pod处于不可用的状态。

### 9.3.5 暂停滚动升级
一个新的pod被创建，与此同时所有的旧的pod还在运行。一旦新的pod成功运行，服务的一部分请求将被切换到新的pod。这样相当于运行了一个金丝雀版本。金丝雀发布是一种可以将应用程序的出错版本和影响到的用户的风险化为最小的技术。与其向每个用户发布新版本，不如用新版本替换一个或者一小部分的pod这种方式。
```
kubectl set image deployment kubia nodejs=lukia/kubia:v4
kubectl rollout pause deployment kubia
```
**恢复自动升级**
```
kubectl rollout resume deployment kubia
```
目前想用金丝雀发布的正确方式是使用两个不同的Deployment并同时调整他们对应的pod数量。

### 9.3.6 阻止出错版本的滚动升级
**了解minReadySeconds**的用处。
minReadySeconds决定了一个pod创建多久后，至少要成功运行多久之后，才能将其视为可用。
kubia-deployment-v3-with-readinesscheck.yaml
```
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  minReadySeconds: 10
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v3
        name: nodejs
        readinessProbe:
          periodSeconds: 1
          httpGet:
            path: /
            port: 8080
```
上面设置了minReadySeconds需要运行10秒，才能视为成功运行的pod.
maxUnavailable设置了能容忍的不可用的pod数为0
periodSeconds设置就绪探针每1秒钟执行一次
**为滚动升级配置deadline**
ProgressDeadlineSeconds指定失败时间。
**取消出错版本的滚动升级**
```
kubectl rollout undo deployment kubia
```