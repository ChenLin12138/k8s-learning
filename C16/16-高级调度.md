# 16.高级调度
k8s允许你去影响pod被调度到哪个节点。起初这只能通过在pod规范里指定节点选择器来实现。
## 16.1 使用污点和容忍度阻止节点调度到特定节点
节点污点和pod对于节点污点的容忍度。这是高级调度的两个很重要的特性。
### 16.1.1 介绍污点和容忍度
污点信息是配置在节点上的
```
k describe node master.k8s
```
Taints表达的事一个污点。他的表现形式为<key>=<value>:<effect>
master:NoSchedule表达的事一个key=master的，value=null,effect=Noschedule
**显示pod的污点容忍度**
容忍度信息是配置在pod上的。
```
k describe po kube-proxy-80wqm -n kube-system
```
**了解污点的效果**
NoSchedule表示如果pod没有容忍这个污点，pod就不能被调度打包含这些污点的节点上。
PreferNoSchedule是NoSchedule的一个宽松版本，表示尽量阻止pod被调度到这个节点上，但是如果没有其他节点可以调度，pod依然会被调度到这个节点上。
NoExecute不同于NoSchedule以及PreferenceNoSchedule，后两者只在调度期间起作用，而NoExecute也会影响节点在运行pod。如果在一个节点上添加NoExecute污点，那么在该节点运行着的pod，如果没有容忍这个NoExecute污点，将会从这个节点移除。
### 16.1.2 在节点添加自定义污点
我们想实现一个规则，非生产环境的pod不能运行在生产环境的节点上。那么我们就可以在生产环境的节点上添加污点来实现。
在节点node1.k8s上添加污点node-type=production:NoSchedule,表示不能容忍node-type=production污点的不能调度到这个节点上。
```
k taint node node1.k8s node-type=production:NoSchedule
```
### 16.1.3 在pod上添加污点容忍度
为了让生产环境的pod能够调度到node1.k8s上。我们需要在生产环境的pod上添加node-type=production的容忍度。
production-deployment.yaml
设置tolerations:key: node-type value: production effect: NoSchedule。那么这个deployment就可以调度到node1.k8s节点上。
```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: prod
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: prod
    spec:
      containers:
      - args:
        - sleep
        - "99999"
        image: busybox
        name: main
      tolerations:
      - key: node-type
        operator: Equal
        value: production
        effect: NoSchedule
```
### 16.1.4 了解污点和污点容忍度的使用场景
节点可以拥有多个污点信息，而pod也可以拥有多个污点容忍度。 pod定义容忍度的时候还可以实现，比如pod可以等待节点属于notReady或者unreachable的状态300秒
```
tolerations:
 - effect : NoExecute
   key: node-type/notReady
   operator: Exists
   value: production
   tolerationsSeconds : 300
 - effect : NoExecute
   key: node-type/unreachable
   operator: Exists
   value: production
   tolerationsSeconds : 300
```
## 16.2 使用节点亲缘性将pod调度到特定节点上
定义在节点上的污点和pod中的污点容忍度，指定了哪些pod能够调度到有污点的节点上。现在的noed affinit的节点亲缘性机制，允许你通知kubernetes将pod只调度到某几个节点的子集上。
**对比节点亲缘性和节点选择器**
pod中使用nodeSelector字段，节点中必须包含所有pod对应字段中指定的label，才能成为pod调度的目标节点。每个pod的亲缘机制要表达的意思是，我们尽量将pod调度到某些节点上去。
failure-domain.beta.kubernetes.io/region表示节点所在的地理区域
failure-domain.beta.kubernetes.io/zone表示该节点所在的可用性区域(availability zone)
kubernetes.io/host表示节点可用的的主机
### 16.2.1 指定强制性节点亲缘性规则
kubia-gpu-nodeselector.yaml
下面是一个节点选择器的例子，他指定了节点只能调度到包含了gpu=true的节点上
```
apiVersion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
  nodeSelector:
    gpu: "true"
  containers:
  - image: luksa/kubia
    name: kubia
```
kubia-gpu-nodeaffinity.yaml
下面是一个节点亲缘规则的例子，
requiredDuringSchedulingIgnoredDuringExecution指定了这个规则在计划时生效，而不在运行时生效。这意味着，这个pod在节点上运行着，而节点失去了这个gpu的标签，pod并不会被移除。
```
apiVersion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: gpu
            operator: In
            values:
            - "true"
  containers:
  - image: luksa/kubia
    name: kubia
```
**较长的节点亲缘属性名的意义**
requiredDuringSchedulingIgnoredDuringExecution
requiredDuringScheduling
IgnoredDuringExecution
### 16.2.2 调度pod时优先考虑某些节点
节点亲缘性最大的好处就是，当调度某一个pod时候，指定调度器可以优先考虑哪些节点。这个可以通过preferredDuringScheduleingIgnoredDuringExecution字段来实现。
preferred-deployment.yaml
下面配置文件描述了80%的权重，我们选择availability-zone in zone1的节点。
20%的权重我们选择share-type in dedicated的节点。
在测试中使用了5个节点，结果4个都调度到标记了zone1的zone1节点上。有一个调度到了标记了zone2的zone2节点上。为什么不是所有的节点都分配到了zone1上呢？这是因为调度器还是使用了其他优先级函数来决定节点的调度。其中之一就是Selector SpreadPriority函数。
```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: pref
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: pref
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            preference:
              matchExpressions:
              - key: availability-zone
                operator: In
                values:
                - zone1
          - weight: 20
            preference:
              matchExpressions:
              - key: share-type
                operator: In
                values:
                - dedicated
      containers:
      - args:
        - sleep
        - "99999"
        image: busybox
        name: main
```

## 16.3 使用pod亲缘性欲非亲缘性对pod进行协同部署
如果我们有一个前端和后端的pod，我们如果能把他们调度再一个网络中，或者靠的比较近。那么就可以降低延迟，提高新能。可以使用**节点**亲缘性规则来确保这两个pod调度到同一节点，同一个机架，同一个数据中心。但是很麻烦，因为亲缘性的指定还要指定节点，机架，数据中心。我们再研究更简单的方案。

### 16.3.1 使用pod间亲缘性将多个pod部署在同一个节点上
frontend-podaffinity-host.yaml
下面是一个创建前端pod的例子，他表达了他对有app:backend的选择器的节点有亲缘性。
```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: frontend
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                app: backend
      containers:
      - name: main
        image: busybox
        args:
        - sleep
        - "99999"
```
### 16.3.2 将pod部署在同一机柜，可用性区域或者其他地理地域
前面的例子使用了podAffinity将前端和后端的pod放在了同一节点上。但是你可能并不想前后端都在同一节点，而是比较靠近。那么我们可以将他们放在同一个可用区域。
topologyKey:failure-domain.beta.kubernetes.io/zone表达在同一个可用性区域中协同部署pod。
topologyKey:failure-domain.beta.kubernetes.io/region表达在同一个地域中协同部署pod
### 16.3.3 表达pod亲缘性优先级取代强制性要求
preferreDuringSchedulingIgnoredDuringExecution:
weight:80表达了亲缘优先级的权重。
frontend-podaffinity-preferred-host.yaml
```


apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: frontend
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app: backend
      containers:
      - name: main
        image: busybox
        args:
        - sleep
        - "99999"
```
### 16.3.4 利用pod的非亲缘性分开调度pod
有时候我们的需求是想让两个节点相互远离。这种特性叫非亲缘性。他把关键字从podAffinity换成了podAntiAffinity。这将让调度器永远不会包含那些有podAntiAffinity匹配标签的pod在同一个节点。
比如我们有2个节点，他们放在一起性能就会相互影响。
**使用非亲缘性分散一个部署中的pod**
frontend-podantiaffinity-host.yaml
```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: frontend
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                app: frontend
      containers:
      - name: main
        image: busybox
        args:
        - sleep
        - "99999"
```