# 4.副本机制和其它控制器:部署托管的pod
一般我们使用ReplicationController或者Deployment这样的资源，由他们来管理实际的pod。如果pod未被托管，如果整个节点失败，节点上的pod会丢失，并不会被新的节点替换。除非这些pod由前面提到的RC或者类似的资源管理。
## 4.1 保持pod健康
### 4.1.1 介绍存活探针
### 4.1.2 创建基于http的存活探针
创建一个web应用，当我们访问5次之后，就返回500.
kubia-liveness-probe.yaml
```
apiVersion: v1
kind: Pod
metadata:
  name: kubia-liveness
spec:
  containers:
  - image: luksa/kubia-unhealthy
    name: kubia
    livenessProbe:
      httpGet:
        path: /
        port: 8080
```
创建po
```
$ kubectl create -f kubia-liveness-probe.yaml
pod/kubia-liveness created
```
### 4.1.3 使用存活探针
restarts是1,表示已经重启过一次了。
```
kubectl get po kubia-liveness
NAME             READY   STATUS    RESTARTS       AGE
kubia-liveness   1/1     Running   1 (108s ago)   6m48s
```
通过describe的event了解为什么必须重启容器
```
kubectl describe po kubia-liveness
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  8m44s                 default-scheduler  Successfully assigned default/kubia-liveness to minikube
  Normal   Pulled     5m32s                 kubelet            Successfully pulled image "luksa/kubia-unhealthy" in 3m11.447272162s
  Normal   Pulled     3m41s                 kubelet            Successfully pulled image "luksa/kubia-unhealthy" in 3.459058046s
  Normal   Created    111s (x3 over 5m31s)  kubelet            Created container kubia
  Normal   Started    111s (x3 over 5m31s)  kubelet            Started container kubia
  Normal   Pulled     111s                  kubelet            Successfully pulled image "luksa/kubia-unhealthy" in 3.340577401s
  Warning  Unhealthy  34s (x9 over 4m34s)   kubelet            Liveness probe failed: HTTP probe failed with statuscode: 500
  Normal   Killing    34s (x3 over 4m14s)   kubelet            Container kubia failed liveness probe, will be restarted
  Normal   Pulling    4s (x4 over 8m43s)    kubelet            Pulling image "luksa/kubia-unhealthy"
  Normal   Pulled     1s                    kubelet            Successfully pulled image "luksa/kubia-unhealthy" in 3.496888364s
```
在底部列出事件显示了容器为什么终止-k8s发现容器不健康，所以终止并重启。
### 4.1.4 配置存活探针的附加属性
在kubectl describe po kubia-liveness可以看到下面信息，
delay=0s表达容器启动后立刻开始侦测
timeout=1s表示容器必须在1秒内响应
period=10s表示10秒内探测一次容器
```
Liveness:       http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3
```
具有初始延迟的存活探针
```
apiVersion: v1
kind: pod
metadata:
  name: kubia-liveness
spec:
  containers:
  - image: luksa/kubia-unhealthy
    name: kubia
    livenessProbe:
      httpGet:
        path: /
        port: 8080
      initialDelaySeconds: 15
```
## 4.2 了解ReplicationController
### 4.2.2 创建一个ReplicationController
kubia-rc.yaml文件中,templae:metadata:labels:app:kubia的名字必须和spec:selector:app:kubia的名字相同，因为rc是根据标签来启动pod的数量的，否则控制器会无休止的创建pod

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: scorpionchenlin/kubia
        ports:
        - containerPort: 8080
```
通过文件kubia-rc.yaml创建ReplicationController
```
kubectl create -f kubia-rc.yaml
```
### 4.2.3 使用ReplicationController
rc启动了3个pod
```
[@ZDMdeMacBook-Pro:4]$ kubectl get po
NAME             READY   STATUS    RESTARTS        AGE
kubia-2clbj      1/1     Running   0               21s
kubia-85z78      1/1     Running   0               21s
kubia-tb2xr      1/1     Running   0               21s
```
手动删除其中一个po后，系统rc自动启动一个新的pod
```
$ kubectl delete po  kubia-tb2xr
pod "kubia-tb2xr" deleted
$ kubectl get po
NAME          READY   STATUS    RESTARTS   AGE
kubia-2clbj   1/1     Running   0          3m46s
kubia-85z78   1/1     Running   0          3m46s
kubia-jr72f   1/1     Running   0          52s
```
**获取有关ReplicationController的信息**
```
kubectl get rc
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         3       5m15s
```
查看这个rc下面pod实例的实际数量和目标数量，每种状态下pod的数量这这个rc有关的时间信息。
```
kubectl describe rc kubia
```
**控制器如何创建新的pod**
控制器通过关键一个新的替代pod来响应旧pod的删除操作。

### 4.2.4 将pod移入或者移除rc的作用域
由rc创建出来的pod并不是绑定到rc上的。他们是通过标签与选择器关联起来的。在任何时刻，rc管理与标签选择器匹配pod。通过改变pod标签，可以将它从rc的作用域中添加和删除。甚至可以从一个作用域移动到另外一个。
如果更改一个pod的标签，使它不在受rc的标签选择器所匹配。那么这个pod就变得和其它手动创建的pod一样了。他不再被任何东西管理如果这个pod运行异常。那么它不会被重新调度。但请记住当更改一个标签时，rc会发现一个pod丢失了，并会创建另外的标签替换它。
```
kubectl label pod kubia-2clbj type=special
kubectl get po --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-2clbj   1/1     Running   0          20m   app=kubia,type=special
kubia-85z78   1/1     Running   0          20m   app=kubia
kubia-jr72f   1/1     Running   0          17m   app=kubia
```
**更改已托管的pod的标签**
修改了，标签之后，一个新的节点启动起来了，老节点被移除了rc的控制域
```
kubectl label pod kubia-2clbj app=foo --overwrite
kubectl get po --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-2clbj   1/1     Running   0          22m   app=foo,type=special
kubia-4dmhv   1/1     Running   0          15s   app=kubia
kubia-85z78   1/1     Running   0          22m   app=kubia
kubia-jr72f   1/1     Running   0          19m   app=kubia
```
### 4.2.6 水平缩放pod
**rc扩容**
```
kubectl scale rc kubia --replicas=10
```
也可以使用这办法,通过修改pod模板来实现，修改replicas to 5
```
kubectl edit rc kubia
$ kubectl get po
NAME          READY   STATUS    RESTARTS   AGE
kubia-4dmhv   1/1     Running   0          12m
kubia-85z78   1/1     Running   0          35m
kubia-g5d8c   1/1     Running   0          89s
kubia-jr72f   1/1     Running   0          32m
kubia-nn5pk   1/1     Running   0          89s
```
描述符从3扩容到了5
```
$ kubectl get rc
NAME    DESIRED   CURRENT   READY   AGE
kubia   5         5         5       35m
```
**用kubectl scale命令缩容**
```
kubectl scale rc kubia --replicas=2
kubectl get rc
NAME    DESIRED   CURRENT   READY   AGE
kubia   2         2         2       37m
```

### 4.2.7 删除一个rc
删除一个rc时，pod也会被删除。但是从本质上来说rc创建出来的pod并不是rc的一部分。所以其实是可以实现删除rc，但是pod继续保持运行的。
删除rc，但是保留pod
```
$ kubectl delete rc kubia --cascade=false
replicationcontroller "kubia" deleted
kubectl get po
NAME          READY   STATUS    RESTARTS   AGE
kubia-85z78   1/1     Running   0          43m
kubia-jr72f   1/1     Running   0          40m
```
## 4.3 使用ReplicaSet而不是ReplicationController
ReplicaSet是ReplicationController的替代组件，他们有着一样的功能。
### 4.3.1 比较ReplicaSet和ReplicationController
ReplicaSet的行为和ReplicationController是完全相同的，但是ReplicaSet的pod选择器的表达能力更强。
ReplicationController选择器：只允许某个标签的匹配pod
ReplicaSet选择器：支持匹配缺少某个标签的pod,包含特定标签的pod不管其值如何
### 4.3.2 定义ReplicaSet
kubia-replicaset.yaml,定义一个ReplicaSet,你应该注意到matchLabels是app: kubia
```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: scorpionchenlin/kubia
```
创建ReplicaSet,其实现在本质是rc管理了2个pod,rs管理了3个pod,其中两个是重合的
```
kubectl create -f kubia-replicaset.yaml
$ kubectl get po
NAME          READY   STATUS    RESTARTS   AGE
kubia-7fm5l   1/1     Running   0          92s
kubia-85z78   1/1     Running   0          60m
kubia-jr72f   1/1     Running   0          57m
```
### 4.3.3 创建和检查ReplicaSet
```
$ kubectl get rs
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         3       4m43s
```
### 4.3.4 使用ReplicaSet的更附有表达力的标签选择器
### 4.3.5 ReplicaSet小结
记得始终使用ReplicaSet而不是ReplicationController,现在我们使用下面命令结束rs
```
[@ZDMdeMacBook-Pro:4]$ kubectl delete rs kubia
replicaset.apps "kubia" deleted
```
## 4.4 使用DaemonSet在节点上运行pod
### 4.4.1 使用DaemonSet在每一个节点上运行一个pod
当一个节点下线时，DaemonSet不会在其他地方创建新的pod，但是当一个新节点添加到集群中时，DaemonSet会立刻部署一个新的pod实例。如果有人无意中删除一个pod,那么它会重新创建一个新的pod。
### 4.4.2 使用DaemonSet只在特定节点上运行pod
可以指定DaemonSet只在特定节点上运行。比如你只想在标记了ssd标签的节点上运行。例如你有一个ssd-monitor的守护进程。
**创建一个DaemonSet YAML定义文件**
ssd-monitor-daemonset.yaml
```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: main
        image: luksa/ssd-monitor
```
**创建DaemonSet**
```
[@ZDMdeMacBook-Pro:4]$ kubectl create -f ssd-monitor-daemonset.yaml
daemonset.apps/ssd-monitor created
[@ZDMdeMacBook-Pro:4]$ kubectl get ds
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ssd-monitor   0         0         0       0            0           disk=ssd        48s
[@ZDMdeMacBook-Pro:4]$ kubectl get po
No resources found in default namespace.
```
这有问题，因为一个po都没知道，问题在哪里呢？是因为忘记给节点打上disk=ssd标签了。
**向节点上添加所需要的标签**
```
[@ZDMdeMacBook-Pro:4]$ kubectl get nodes
NAME       STATUS   ROLES                  AGE   VERSION
minikube   Ready    control-plane,master   10d   v1.22.1
```
现在像这样给节点添加disk=ssd标签
```
[@ZDMdeMacBook-Pro:4]$ kubectl label node minikube disk=ssd
node/minikube labeled
```
再看看po是否启动
```
[@ZDMdeMacBook-Pro:4]$ kubectl get po
NAME                READY   STATUS    RESTARTS   AGE
ssd-monitor-fqm9c   1/1     Running   0          28s
```
如果我们有多个节点都打上了disk=ssd标签，那么每一个节点都会启动一个ssd-monitor的pod
**从节点上删除所需的标签**
```
[@ZDMdeMacBook-Pro:4]$ kubectl label node minikube disk=hdd --overwrite
node/minikube labeled
[@ZDMdeMacBook-Pro:4]$ kubectl get po
NAME                READY   STATUS        RESTARTS   AGE
ssd-monitor-fqm9c   1/1     Terminating   0          3m29s
```
pod如预期种植了，如果删除ssd-monitor DaemonSet，那么其它属于他的pod也会被删除。
## 4.5 运行执行单个任务的pod
前面提到的ReplicationController, ReplicaSet 和 DaemonSet会持续运行任务，永远都不会完成。那么我们想执行一个一次完成了就停止的任务呢？
### 4.5.1 介绍Job资源
Job会是一种资源，他管理的pod在完成任务之前出现了问题，他会启动一个新的pod继续工作，直到pod中的任务完成。
### 4.5.2 定义Job资源
exporter.yaml,需要注意的是kind:job, apiVersion:batch/v1,restartPolicy: OnFailure
注意job不能使用restartPolicy的默认策略Always,因为我们并不是要无限的重启。
```
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      # 只在进程异常时重启pod
      restartPolicy: OnFailure
      containers:
      - name: main
        # sleep 120s后自动退出
        image: luksa/batch-job
```
完成job创建
```
[@ZDMdeMacBook-Pro:4]$ kubectl create -f exporter.yaml
job.batch/batch-job created
```

### 4.5.3 看Job运行一个pod
```
[@ZDMdeMacBook-Pro:4]$ kubectl get jobs
NAME        COMPLETIONS   DURATION   AGE
batch-job   0/1           61s        61s
[@ZDMdeMacBook-Pro:4]$ kubectl get po
NAME                 READY   STATUS    RESTARTS   AGE
batch-job--1-8vfcp   1/1     Running   0          80s
```
job完成后可以查询日志
```
[@ZDMdeMacBook-Pro:4]$ kubectl logs batch-job--1-8vfcp
Sun Sep 26 10:04:18 UTC 2021 Batch job starting
Sun Sep 26 10:06:17 UTC 2021 Finished succesfully
```
查看job状态
```
[@ZDMdeMacBook-Pro:4]$ kubectl get job
NAME        COMPLETIONS   DURATION   AGE
batch-job   1/1           2m12s      3m16s
```




















